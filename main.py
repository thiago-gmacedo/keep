#!/usr/bin/env python3
"""
OCR Keep ‚Üí Obsidian + Vector DB v1.0.0
Main Pipeline - Centralizador de todo o fluxo de processamento de notas manuscritas

Este m√≥dulo executa o pipeline completo:
1. Conecta ao Google Keep
2. Busca novas imagens (n√£o processadas) 
3. Executa OCR
4. Estrutura com LLM
5. Gera arquivos .md no padr√£o Obsidian
6. Indexa no ChromaDB
7. Move imagens processadas

Autor: Thiago Macedom
Data: 29/05/2025
Vers√£o: 1.0.0
"""

import sys
import os
import json
import shutil
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

# Adicionar diret√≥rio raiz ao path para importa√ß√µes
ROOT_DIR = Path(__file__).parent
sys.path.insert(0, str(ROOT_DIR))

# Importa√ß√µes dos m√≥dulos existentes
try:
    # Importar fun√ß√µes de conectividade e credenciais do Google Keep
    from ocr_extractor import (
        connect_to_keep, 
        load_keep_credentials,
        transcribe_handwriting,
        download_blob,
        encode_image_to_base64
    )
    
    # Importar m√≥dulo de gera√ß√£o Obsidian
    from src.obsidian_writer import json_to_obsidian, validate_json_structure
    
    # Importar m√≥dulo de indexa√ß√£o ChromaDB
    from src.chroma_indexer import index_note_in_chroma
    
    print("‚úÖ Todos os m√≥dulos importados com sucesso")
    
except ImportError as e:
    print(f"‚ùå Erro ao importar m√≥dulos: {e}")
    print("Certifique-se de que todos os m√≥dulos est√£o no local correto")
    sys.exit(1)

# Informa√ß√µes de vers√£o
__version__ = "1.0.0"
__author__ = "Thiago Macedom"
__date__ = "29/05/2025"

# Configura√ß√µes
IMAGES_DIR = ROOT_DIR / "images"
PROCESSED_DIR = IMAGES_DIR / "processed"
OBSIDIAN_DIR = ROOT_DIR / "obsidian_notes"
PROCESSED_NOTES_FILE = ROOT_DIR / ".processed_notes.json"

# Criar diret√≥rios necess√°rios
IMAGES_DIR.mkdir(exist_ok=True)
PROCESSED_DIR.mkdir(exist_ok=True)
OBSIDIAN_DIR.mkdir(exist_ok=True)


def setup_api_keys():
    """Configura as chaves de API necess√°rias"""
    print("üîë Verificando configura√ß√£o de APIs...")
    
    # Carregar credenciais do arquivo de configura√ß√£o
    config = load_keep_credentials()
    
    # Verificar chave OpenAI
    openai_key = config.get('OPENAI_API_KEY') or os.environ.get('OPENAI_API_KEY')
    if not openai_key or openai_key.startswith('sua-chave'):
        raise ValueError("‚ùå Chave da API OpenAI n√£o configurada no arquivo .env/config")
    
    # Configurar OpenAI
    try:
        import openai
        openai.api_key = openai_key
    except ImportError:
        print("‚ö†Ô∏è OpenAI n√£o instalado, mas chave configurada")
    
    # Verificar credenciais Google Keep
    email = config.get('GOOGLE_EMAIL')
    master_token = config.get('GOOGLE_MASTER_TOKEN')
    
    if not email or not master_token:
        raise ValueError("‚ùå Credenciais do Google Keep n√£o configuradas no arquivo .env/config")
    
    print("‚úÖ APIs configuradas com sucesso")
    return config


def load_processed_notes() -> Dict[str, List[str]]:
    """Carrega a lista de IDs de notas j√° processadas"""
    if not PROCESSED_NOTES_FILE.exists():
        return {}
    
    try:
        with open(PROCESSED_NOTES_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao carregar registro de notas processadas: {e}")
        return {}


def save_processed_note(note_id: str, label_name: str = "main_pipeline"):
    """Adiciona uma nota ao registro de notas processadas"""
    processed_notes = load_processed_notes()
    
    if label_name not in processed_notes:
        processed_notes[label_name] = []
    
    if note_id not in processed_notes[label_name]:
        processed_notes[label_name].append(note_id)
    
    try:
        with open(PROCESSED_NOTES_FILE, 'w', encoding='utf-8') as f:
            json.dump(processed_notes, f, indent=2, ensure_ascii=False)
        print(f"üìù Nota {note_id[:8]} registrada como processada")
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao salvar registro: {e}")


def is_note_processed(note_id: str, label_name: str = "main_pipeline") -> bool:
    """Verifica se uma nota j√° foi processada"""
    processed_notes = load_processed_notes()
    return (label_name in processed_notes and 
            note_id in processed_notes[label_name])


def get_new_notes_with_images(keep, label_name: Optional[str] = None) -> List[Any]:
    """
    Busca notas de HOJE com imagens que ainda n√£o foram processadas
    
    Args:
        keep: Inst√¢ncia conectada do Google Keep
        label_name: Nome da label para filtrar (opcional)
    
    Returns:
        Lista de notas de hoje n√£o processadas com anexos de imagem
    """
    from datetime import timezone
    
    # Data atual (UTC) - filtra apenas notas de hoje
    hoje = datetime.now(timezone.utc).date()
    print(f"üîç Buscando notas de HOJE ({hoje.strftime('%d/%m/%Y')}) com imagens n√£o processadas...")
    
    try:
        if label_name:
            print(f"üè∑Ô∏è Filtrando por label: {label_name}")
            label = keep.findLabel(label_name)
            if not label:
                print(f"‚ö†Ô∏è Label '{label_name}' n√£o encontrada")
                return []
            notes = list(keep.find(labels=[label]))
        else:
            print("üìã Buscando em todas as notas")
            notes = list(keep.all())
        
        print(f"üìä Total de notas encontradas: {len(notes)}")
        
        # Filtrar notas de HOJE
        notes_today = [note for note in notes if note.timestamps.updated.date() == hoje]
        print(f"üìÖ Notas de hoje: {len(notes_today)}")
        
        # Filtrar notas com anexos
        notes_with_blobs = []
        for note in notes_today:
            if hasattr(note, 'blobs') and note.blobs:
                notes_with_blobs.append(note)
        
        print(f"üìé Notas de hoje com anexos: {len(notes_with_blobs)}")
        
        # Filtrar notas n√£o processadas
        new_notes = []
        pipeline_label = f"main_pipeline_{label_name}" if label_name else "main_pipeline"
        
        for note in notes_with_blobs:
            if not is_note_processed(note.id, pipeline_label):
                new_notes.append(note)
        
        print(f"üÜï Notas de hoje novas para processar: {len(new_notes)}")
        return new_notes
        
    except Exception as e:
        print(f"‚ùå Erro ao buscar notas: {e}")
        return []


def download_note_images(keep, note) -> List[Path]:
    """
    Baixa todas as imagens de uma nota
    
    Args:
        keep: Inst√¢ncia do Google Keep
        note: Nota do Google Keep
    
    Returns:
        Lista de caminhos das imagens baixadas
    """
    print(f"üì• Baixando imagens da nota: {note.title or 'Sem t√≠tulo'}")
    
    downloaded_images = []
    
    for i, blob in enumerate(note.blobs):
        try:
            print(f"üì∑ Processando anexo {i+1}/{len(note.blobs)}...")
            
            # Definir keep como vari√°vel global temporariamente para compatibilidade
            import ocr_extractor
            ocr_extractor.keep = keep
            
            # Baixar o blob usando a fun√ß√£o existente
            img_path = download_blob(blob, note.title or "sem_titulo", i)
            
            if img_path and img_path.exists():
                # Mover para o diret√≥rio correto se necess√°rio
                if img_path.parent != IMAGES_DIR:
                    new_path = IMAGES_DIR / img_path.name
                    shutil.move(str(img_path), str(new_path))
                    img_path = new_path
                
                print(f"‚úÖ Imagem salva: {img_path}")
                downloaded_images.append(img_path)
            else:
                print(f"‚ùå Falha ao baixar anexo {i+1}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao processar anexo {i+1}: {e}")
            continue
    
    return downloaded_images


def process_image_ocr(image_path: Path) -> str:
    """
    Executa OCR em uma imagem
    
    Args:
        image_path: Caminho da imagem
    
    Returns:
        Texto extra√≠do da imagem
    """
    print(f"üîç Executando OCR em: {image_path.name}")
    
    try:
        # Validar que √© uma imagem
        from PIL import Image
        with Image.open(image_path) as img:
            print(f"üìä Imagem validada - Formato: {img.format}, Tamanho: {img.size}")
        
        # Executar OCR usando a fun√ß√£o existente
        extracted_text = transcribe_handwriting(str(image_path))
        
        print(f"‚úÖ OCR conclu√≠do - {len(extracted_text)} caracteres extra√≠dos")
        return extracted_text
        
    except Exception as e:
        print(f"‚ùå Erro no OCR: {e}")
        raise


def parse_text_to_json(text: str) -> Optional[Dict[str, Any]]:
    """
    Converte texto extra√≠do em JSON estruturado
    
    Args:
        text: Texto extra√≠do do OCR
    
    Returns:
        Dicion√°rio JSON estruturado ou None se falhar
    """
    print("üß† Estruturando texto com LLM...")
    
    try:
        # Tentar extrair JSON do texto (pode vir com markdown code blocks)
        import re
        
        # Primeiro, tentar encontrar blocos de c√≥digo JSON
        json_match = re.search(r'```(?:json)?\s*\n?(.*?)\n?```', text, re.DOTALL | re.IGNORECASE)
        if json_match:
            json_content = json_match.group(1).strip()
        else:
            # Se n√£o h√° code blocks, usar o texto inteiro
            json_content = text.strip()
        
        # Tentar fazer parse do JSON
        json_data = json.loads(json_content)
        
        # Validar estrutura b√°sica
        required_fields = ['title', 'summary', 'keywords', 'tasks', 'notes', 'reminders']
        for field in required_fields:
            if field not in json_data:
                json_data[field] = [] if field in ['keywords', 'tasks', 'notes', 'reminders'] else ""
        
        print("‚úÖ Texto estruturado com sucesso em JSON")
        return json_data
        
    except json.JSONDecodeError:
        print("‚ö†Ô∏è N√£o foi poss√≠vel estruturar como JSON - salvando como texto puro")
        return None
    except Exception as e:
        print(f"‚ùå Erro na estrutura√ß√£o: {e}")
        return None


def save_json_data(json_data: Dict[str, Any], image_path: Path) -> Path:
    """
    Salva dados JSON estruturados
    
    Args:
        json_data: Dados estruturados
        image_path: Caminho da imagem original
    
    Returns:
        Caminho do arquivo JSON salvo
    """
    json_path = image_path.with_suffix('.json')
    
    try:
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ JSON salvo: {json_path}")
        return json_path
        
    except Exception as e:
        print(f"‚ùå Erro ao salvar JSON: {e}")
        raise


def save_text_data(text: str, image_path: Path) -> Path:
    """
    Salva texto puro (fallback quando JSON falha)
    
    Args:
        text: Texto extra√≠do
        image_path: Caminho da imagem original
    
    Returns:
        Caminho do arquivo de texto salvo
    """
    text_path = image_path.with_suffix('.txt')
    
    try:
        with open(text_path, 'w', encoding='utf-8') as f:
            f.write(text)
        
        print(f"üìÑ Texto salvo: {text_path}")
        return text_path
        
    except Exception as e:
        print(f"‚ùå Erro ao salvar texto: {e}")
        raise


def generate_obsidian_note(json_data: Dict[str, Any]) -> bool:
    """
    Gera arquivo Markdown para Obsidian
    
    Args:
        json_data: Dados estruturados
    
    Returns:
        True se gera√ß√£o foi bem-sucedida
    """
    print("üìö Gerando nota Obsidian...")
    
    try:
        # Validar estrutura do JSON
        if not validate_json_structure(json_data):
            print("‚ö†Ô∏è Estrutura JSON inv√°lida para Obsidian")
            return False
        
        # Gerar arquivo Obsidian
        json_to_obsidian(json_data, str(OBSIDIAN_DIR))
        print("‚úÖ Nota Obsidian gerada com sucesso")
        return True
        
    except Exception as e:
        print(f"‚ùå Erro ao gerar nota Obsidian: {e}")
        return False


def index_in_chromadb(json_data: Dict[str, Any]) -> bool:
    """
    Indexa dados no ChromaDB para busca sem√¢ntica
    
    Args:
        json_data: Dados estruturados
    
    Returns:
        True se indexa√ß√£o foi bem-sucedida
    """
    print("üß† Indexando no ChromaDB...")
    
    try:
        success = index_note_in_chroma(json_data)
        if success:
            print("‚úÖ Dados indexados no ChromaDB com sucesso")
        else:
            print("‚ö†Ô∏è Falha na indexa√ß√£o no ChromaDB")
        return success
        
    except Exception as e:
        print(f"‚ùå Erro na indexa√ß√£o ChromaDB: {e}")
        return False


def move_processed_image(image_path: Path) -> bool:
    """
    Move imagem processada para o diret√≥rio processed/
    
    Args:
        image_path: Caminho da imagem processada
    
    Returns:
        True se movimenta√ß√£o foi bem-sucedida
    """
    try:
        processed_path = PROCESSED_DIR / image_path.name
        shutil.move(str(image_path), str(processed_path))
        print(f"üìÅ Imagem movida para: {processed_path}")
        return True
        
    except Exception as e:
        print(f"‚ùå Erro ao mover imagem: {e}")
        return False


def process_single_image(image_path: Path, note_id: str = None) -> Dict[str, bool]:
    """
    Processa uma √∫nica imagem atrav√©s de todo o pipeline
    
    Args:
        image_path: Caminho da imagem
        note_id: ID da nota (opcional)
    
    Returns:
        Dicion√°rio com status de cada etapa
    """
    print(f"\n{'='*60}")
    print(f"üîÑ Processando imagem: {image_path.name}")
    print(f"{'='*60}")
    
    results = {
        'ocr': False,
        'json_parsing': False,
        'obsidian': False,
        'chromadb': False,
        'move_image': False
    }
    
    try:
        # Etapa 1: OCR
        extracted_text = process_image_ocr(image_path)
        results['ocr'] = True
        
        # Etapa 2: Estrutura√ß√£o em JSON
        json_data = parse_text_to_json(extracted_text)
        
        if json_data:
            # Salvar JSON
            save_json_data(json_data, image_path)
            results['json_parsing'] = True
            
            # Etapa 3: Gerar nota Obsidian
            if generate_obsidian_note(json_data):
                results['obsidian'] = True
            
            # Etapa 4: Indexar no ChromaDB
            if index_in_chromadb(json_data):
                results['chromadb'] = True
        else:
            # Fallback: salvar como texto puro
            save_text_data(extracted_text, image_path)
            print("üíæ Conte√∫do salvo como texto puro")
        
        # Etapa 5: Mover imagem processada
        if move_processed_image(image_path):
            results['move_image'] = True
        
        print(f"‚úÖ Processamento de {image_path.name} conclu√≠do")
        
    except Exception as e:
        print(f"‚ùå Erro no processamento de {image_path.name}: {e}")
    
    return results


def run_pipeline(label_name: Optional[str] = None):
    """
    Executa o pipeline completo de processamento
    
    Args:
        label_name: Nome da label para filtrar notas (opcional)
    """
    print(f"\n{'='*80}")
    print(f"üöÄ OCR KEEP ‚Üí OBSIDIAN + VECTOR DB v{__version__}")
    print(f"üìù PIPELINE DE PROCESSAMENTO DE NOTAS MANUSCRITAS")
    print(f"üë§ Autor: {__author__} | üìÖ Data: {__date__}")
    print(f"{'='*80}")
    
    start_time = datetime.now()
    
    try:
        # Etapa 1: Configurar APIs
        setup_api_keys()
        
        # Etapa 2: Conectar ao Google Keep
        print("\nüîó Conectando ao Google Keep...")
        keep = connect_to_keep()
        
        # Etapa 3: Buscar novas notas com imagens
        new_notes = get_new_notes_with_images(keep, label_name)
        
        if not new_notes:
            print("‚ÑπÔ∏è Nenhuma nota nova para processar")
            return
        
        # Estat√≠sticas
        total_notes = len(new_notes)
        processed_notes = 0
        failed_notes = 0
        total_images = 0
        processed_images = 0
        
        pipeline_label = f"main_pipeline_{label_name}" if label_name else "main_pipeline"
        
        # Processar cada nota
        for note_idx, note in enumerate(new_notes, 1):
            print(f"\n{'='*60}")
            print(f"üìù Processando nota {note_idx}/{total_notes}")
            print(f"üìã T√≠tulo: {note.title or 'Sem t√≠tulo'}")
            print(f"üÜî ID: {note.id[:8]}...")
            print(f"{'='*60}")
            
            try:
                # Baixar imagens da nota
                images = download_note_images(keep, note)
                total_images += len(images)
                
                if not images:
                    print("‚ö†Ô∏è Nenhuma imagem baixada desta nota")
                    continue
                
                # Processar cada imagem
                note_success = True
                for image_path in images:
                    results = process_single_image(image_path, note.id)
                    
                    # Verificar se pelo menos OCR funcionou
                    if results['ocr']:
                        processed_images += 1
                    else:
                        note_success = False
                
                # Marcar nota como processada se pelo menos uma imagem foi processada
                if note_success and processed_images > 0:
                    save_processed_note(note.id, pipeline_label)
                    processed_notes += 1
                else:
                    failed_notes += 1
                    
            except Exception as e:
                print(f"‚ùå Erro ao processar nota {note.title or 'sem t√≠tulo'}: {e}")
                failed_notes += 1
                continue
        
        # Resumo final
        end_time = datetime.now()
        duration = end_time - start_time
        
        print(f"\n{'='*80}")
        print(f"‚úÖ PIPELINE CONCLU√çDO")
        print(f"{'='*80}")
        print(f"‚è±Ô∏è Dura√ß√£o: {duration}")
        print(f"üìä Estat√≠sticas:")
        print(f"   üìù Notas processadas: {processed_notes}/{total_notes}")
        print(f"   ‚ùå Notas com falha: {failed_notes}")
        print(f"   üñºÔ∏è Imagens processadas: {processed_images}/{total_images}")
        print(f"üìÅ Diret√≥rios:")
        print(f"   üñºÔ∏è Imagens processadas: {PROCESSED_DIR}")
        print(f"   üìö Notas Obsidian: {OBSIDIAN_DIR}")
        print(f"   üß† ChromaDB: ./chroma_db")
        print(f"{'='*80}")
        
    except Exception as e:
        print(f"‚ùå Erro cr√≠tico no pipeline: {e}")
        raise


if __name__ == "__main__":
    """
    Ponto de entrada principal
    Aceita argumentos opcionais para filtrar por label
    """
    
    # Verificar argumentos de linha de comando
    label_filter = None
    if len(sys.argv) > 1:
        if sys.argv[1] in ['--help', '-h']:
            print("\nüìã USO DO PIPELINE:")
            print("  python main.py              # Processar todas as notas")
            print("  python main.py LABEL        # Processar notas com label espec√≠fica")
            print("  python main.py --help       # Exibir esta ajuda")
            print("\nEXEMPLOS:")
            print("  python main.py diario       # Processar notas com label 'diario'")
            print("  python main.py OCR          # Processar notas com label 'OCR'")
            sys.exit(0)
        else:
            label_filter = sys.argv[1]
            print(f"üè∑Ô∏è Filtrando por label: {label_filter}")
    
    try:
        run_pipeline(label_filter)
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Pipeline interrompido pelo usu√°rio")
        sys.exit(1)
    except Exception as e:
        print(f"\nüí• Erro fatal: {e}")
        sys.exit(1)
